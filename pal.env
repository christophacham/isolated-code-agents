# PAL MCP Server Configuration
# Supports both local Ollama and OpenRouter cloud models

# ============================================
# LOCAL OLLAMA CONFIGURATION (Primary)
# ============================================
OLLAMA_ENABLED=true
OLLAMA_BASE_URL=http://localhost:11434
CUSTOM_API_URL=http://localhost:11434
OLLAMA_MODELS_CONFIG=/home/aiuser/pal-mcp-server/conf/custom_models.json

# Default to local Ollama for most tasks
DEFAULT_PROVIDER=ollama
DEFAULT_MODEL=qwen2.5-coder:32b

# ============================================
# OPENROUTER CONFIGURATION (Disabled - local only)
# ============================================
OPENROUTER_ENABLED=false
# OPENROUTER_API_KEY=your-openrouter-api-key-here
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# OPENROUTER_MODELS_CONFIG=/home/aiuser/pal-mcp-server/conf/openrouter_models.json

# ============================================
# TOOL CONFIGURATION
# ============================================
# Enable all tools (empty = all enabled)
DISABLED_TOOLS=

# Tool-specific model preferences
THINKDEEP_MODEL=deepseek-r1:32b
CHALLENGE_MODEL=deepseek-r1:32b
CODEREVIEW_MODEL=qwen2.5-coder:32b
DEBUG_MODEL=qwen2.5-coder:32b
CHAT_MODEL=dolphin3:8b
CONSENSUS_MODELS=deepseek-r1:32b,qwen2.5-coder:32b,qwen3:32b

# ============================================
# SERVER CONFIGURATION
# ============================================
PAL_SERVER_HOST=127.0.0.1
PAL_SERVER_PORT=8765
LOG_LEVEL=INFO

# Context window management
MAX_CONTEXT_TOKENS=128000

# Enable web search for API lookup tool
ENABLE_WEB_SEARCH=true

# Rate limiting (requests per minute)
RATE_LIMIT_RPM=60
